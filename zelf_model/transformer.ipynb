{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b7eec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec37f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feedforward\n",
    "# Simple FeedForward layer. Zelfde als in GPT decoder.\n",
    "@keras.saving.register_keras_serializable()\n",
    "class FeedForward(keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, factor=4, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.factor = factor\n",
    "    self.relu = keras.activations.relu\n",
    "\n",
    "  def build(self, batch_input_shape):\n",
    "    time_steps, embed_size = batch_input_shape[1:]\n",
    "    self.kernel1 = self.add_weight(shape=(embed_size, self.factor*embed_size))\n",
    "    self.bias1 = self.add_weight(shape=(self.factor*embed_size, ),\n",
    "                                 initializer=\"zeros\")\n",
    "    self.kernel2 = self.add_weight(shape=(self.factor*embed_size, embed_size))\n",
    "    self.bias2 = self.add_weight(shape=(embed_size, ),\n",
    "                                 initializer=\"zeros\")\n",
    "\n",
    "  def call(self, inputs):\n",
    "    a =  self.relu(keras.ops.matmul(inputs, self.kernel1) + self.bias1)\n",
    "    return keras.ops.matmul(a, self.kernel2) + self.bias2\n",
    "\n",
    "  def get_config(self):\n",
    "    base_config = super().get_config()\n",
    "    return {\n",
    "        **base_config,\n",
    "        \"factor\": self.factor,\n",
    "    }\n",
    "# EmbeddingWithPosition\n",
    "# Embedding with Position.\n",
    "@keras.saving.register_keras_serializable()\n",
    "class EmbeddingWithPosition(keras.layers.Layer):\n",
    "  \"\"\"\n",
    "  Computes and embedding and also adds a positional embedding.\n",
    "  This layer does not support masking.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, num_tokens, max_seq_length, embed_size, dropout=0, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.num_tokens = num_tokens\n",
    "    self.max_seq_length = max_seq_length\n",
    "    self.embed_size = embed_size\n",
    "    self.dropout = dropout\n",
    "\n",
    "  def build(self, batch_input_shape):\n",
    "    # Shape not needed\n",
    "\n",
    "    self.kernel = self.add_weight(shape=(self.num_tokens, self.embed_size))\n",
    "    self.pos_kernel = self.add_weight(shape=(self.max_seq_length, self.embed_size))\n",
    "    if self.dropout > 0:\n",
    "        self.dropout_layer = keras.layers.Dropout(self.dropout)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    _, length = keras.ops.shape(inputs)\n",
    "\n",
    "    embeddings = keras.ops.take(self.kernel, inputs, axis=0) # (batch, length, embed_size)\n",
    "    pos_embeddings = self.pos_kernel[:length]\n",
    "\n",
    "    s = embeddings + pos_embeddings  # rely on broadcasting. Mask is lost\n",
    "\n",
    "    return s if self.dropout == 0 else self.dropout_layer(s)\n",
    "\n",
    "  def get_config(self):\n",
    "    base_config = super().get_config()\n",
    "    return {\n",
    "        **base_config,\n",
    "        \"num_tokens\": self.num_tokens,\n",
    "        \"max_seq_length\": self.max_seq_length,\n",
    "        \"embed_size\": self.embed_size,\n",
    "        \"dropout\": self.dropout\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa7973f",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c17ad2c",
   "metadata": {},
   "source": [
    "![](../image/encoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fbc690",
   "metadata": {},
   "source": [
    "## init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4956050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, num_heads, embed_size, dropout=0, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.num_heads = num_heads\n",
    "    self.embed_size = embed_size\n",
    "    self.dropout = dropout\n",
    "    if self.dropout > 0:\n",
    "        self.dropout_layer = keras.layers.Dropout(self.dropout)\n",
    "\n",
    "    self.masked_multi_head_attn = keras.layers.MultiHeadAttention(\n",
    "        num_heads=self.num_heads,\n",
    "        key_dim = self.embed_size // self.num_heads\n",
    "    )\n",
    "    self.layer_norm_1 = keras.layers.LayerNormalization()\n",
    "    self.feed_forward = FeedForward()\n",
    "    self.layer_norm_2 = keras.layers.LayerNormalization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a9b53c",
   "metadata": {},
   "source": [
    "## call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9924672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call(self, inputs, attention_mask=None):\n",
    "    skip = inputs\n",
    "    inputs = self.masked_multi_head_attn(inputs, inputs, attention_mask=attention_mask)\n",
    "    if self.dropout != 0:\n",
    "        inputs = self.dropout_layer(inputs)\n",
    "    inputs = self.layer_norm_1(keras.layers.Add()([inputs, skip]))\n",
    "\n",
    "    skip = inputs\n",
    "    inputs = self.feed_forward(inputs)\n",
    "    if self.dropout != 0:\n",
    "        inputs = self.dropout_layer(inputs)\n",
    "    inputs = self.layer_norm_2(keras.layers.Add()([skip, inputs]))\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe91acbf",
   "metadata": {},
   "source": [
    "# get model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c657828b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_encoder_model(num_tokens, max_seq_length, embed_size, num_heads, num_blocks, num_classes, use_mask, scale_embeddings, dropout):\n",
    "\n",
    "  inputs = keras.layers.Input(shape=[max_seq_length], dtype=int)\n",
    "  mask, attention_mask = None, None\n",
    "  \n",
    "  if use_mask:\n",
    "    mask = keras.ops.not_equal(inputs, 0)\n",
    "    attention_mask = keras.ops.expand_dims(mask, axis=1)\n",
    "\n",
    "  embed = EmbeddingWithPosition(num_tokens=num_tokens, max_seq_length=max_seq_length, embed_size=embed_size, dropout=dropout, name=\"pos_embedding\")(inputs)\n",
    "\n",
    "  for index in range(num_blocks):\n",
    "    embed = EncoderBlock(num_heads=num_heads, embed_size=embed_size, dropout=dropout, name=f\"encoder_block_{index}\")(embed, attention_mask)\n",
    "\n",
    "  # Simple classification head\n",
    "  embed = keras.layers.GlobalAveragePooling1D()(embed, mask=mask)\n",
    "  if scale_embeddings:\n",
    "      scale = keras.ops.sqrt(keras.ops.sum(mask, axis=1, keepdims=True))\n",
    "      embed = embed * scale\n",
    "\n",
    "  embed = keras.layers.Dense(units=embed_size, activation='relu')(embed)\n",
    "\n",
    "  if dropout > 0:\n",
    "      embed = keras.layers.Dropout(dropout)(embed)\n",
    "  if num_classes == 1:\n",
    "    output = keras.layers.Dense(units=1, activation='sigmoid')(embed)\n",
    "  else:\n",
    "    output = keras.layers.Dense(units=num_classes, activation='softmax')(embed)\n",
    "\n",
    "  return keras.Model(inputs=inputs, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50990845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_encoder_model(num_tokens: int, max_seq_length: int, embed_size: int, num_heads:int,\n",
    "    num_blocks:int, num_classes:int, use_mask:bool=False, scale_embeddings:bool=False, dropout:float=0):\n",
    "\n",
    "  inputs = keras.layers.Input(shape=[max_seq_length], dtype=int)\n",
    "  mask, attention_mask = None, None\n",
    "  if use_mask:\n",
    "    mask = keras.ops.not_equal(inputs, 0)\n",
    "    attention_mask = keras.ops.expand_dims(mask, axis=1)\n",
    "\n",
    "  embed = EmbeddingWithPosition(num_tokens=num_tokens,\n",
    "                                  max_seq_length=max_seq_length,\n",
    "                                  embed_size=embed_size,\n",
    "                                  dropout=dropout,\n",
    "                                  name=\"pos_embedding\")(inputs)\n",
    "\n",
    "  for index in range(num_blocks):\n",
    "    embed = EncoderBlock(num_heads=num_heads, embed_size=embed_size, dropout=dropout,\n",
    "                         name=f\"encoder_block_{index}\")(embed, attention_mask)\n",
    "\n",
    "  # Simple classification head\n",
    "  embed = keras.layers.GlobalAveragePooling1D()(embed, mask=mask) # (B, embed_size)\n",
    "  if scale_embeddings:\n",
    "      scale = keras.ops.sqrt(keras.ops.sum(mask, axis=1, keepdims=True))\n",
    "      embed = embed * scale\n",
    "\n",
    "  embed = keras.layers.Dense(units=embed_size, activation='relu')(embed)\n",
    "  # Additional dropout just before output layer\n",
    "  if dropout > 0:\n",
    "      embed = keras.layers.Dropout(dropout)(embed)\n",
    "  if num_classes == 1:\n",
    "    output = keras.layers.Dense(units=1, activation='sigmoid')(embed)\n",
    "  else:\n",
    "    output = keras.layers.Dense(units=num_classes, activation='softmax')(embed)\n",
    "\n",
    "  return keras.Model(inputs=inputs, outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd99f675",
   "metadata": {},
   "source": [
    "# decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6dbb0b",
   "metadata": {},
   "source": [
    "encoder-decoder attention zit niet bij een enkel decoder model aangezien het input van de encoder nodig heeft\n",
    "\n",
    "![](../image/decoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f848111",
   "metadata": {},
   "source": [
    "decoder bestaat uit:\n",
    "- masked multi head attention\n",
    "- layer normalization\n",
    "- feedforward layer\n",
    "- layer normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a2a3b0",
   "metadata": {},
   "source": [
    "## init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc76d2b",
   "metadata": {},
   "source": [
    "init is de achitectuur van het model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71916f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, num_heads, embed_size, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.num_heads = num_heads\n",
    "    self.embed_size = embed_size\n",
    "\n",
    "    self.masked_multi_head_attn = keras.layers.MultiHeadAttention(\n",
    "        num_heads=self.num_heads,\n",
    "        key_dim = self.embed_size // self.num_heads\n",
    "    )\n",
    "    \n",
    "    self.layer_norm_1 = keras.layers.LayerNormalization()\n",
    "    self.feed_forward = FeedForward()\n",
    "    self.layer_norm_2 = keras.layers.LayerNormalization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8975d1",
   "metadata": {},
   "source": [
    "## call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63530ee3",
   "metadata": {},
   "source": [
    "dit is hoe de data door de architectuur stroomt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3390bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call(self, inputs):\n",
    "\n",
    "    # dit is voor skip\n",
    "    skip = inputs\n",
    "    inputs = self.masked_multi_head_attn(inputs, inputs, use_causal_mask=True)\n",
    "    inputs = self.layer_norm_1(keras.layers.Add()([inputs, skip]))\n",
    "\n",
    "    skip = inputs\n",
    "    inputs = self.feed_forward(inputs)\n",
    "    inputs = self.layer_norm_2(keras.layers.Add()([skip, inputs]))\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658f2a15",
   "metadata": {},
   "source": [
    "# encoder-decoder decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d31d011",
   "metadata": {},
   "source": [
    "![](../image/decoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d564e90",
   "metadata": {},
   "source": [
    "decoder bestaat uit:\n",
    "- masked multi head attention\n",
    "- layer normalization\n",
    "- encoder-decoder attention\n",
    "- layer normalization\n",
    "- feedforward layer\n",
    "- layer normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa597847",
   "metadata": {},
   "source": [
    "## init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be772a9a",
   "metadata": {},
   "source": [
    "init is de achitectuur van het model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87da2bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, num_heads, embed_size, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.num_heads = num_heads\n",
    "    self.embed_size = embed_size\n",
    "\n",
    "    self.masked_multi_head_attn = keras.layers.MultiHeadAttention(\n",
    "        num_heads=self.num_heads,\n",
    "        key_dim = self.embed_size // self.num_heads\n",
    "    )\n",
    "    self.layer_norm_1 = keras.layers.LayerNormalization()\n",
    "\n",
    "    self.cross_attn = keras.layers.MultiHeadAttention(\n",
    "        num_heads=self.num_heads,\n",
    "        key_dim = self.embed_size // self.num_heads\n",
    "    )\n",
    "    self.layer_norm_2 = keras.layers.LayerNormalization()\n",
    "\n",
    "    self.feed_forward = FeedForward()\n",
    "    self.layer_norm_3 = keras.layers.LayerNormalization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c56150b",
   "metadata": {},
   "source": [
    "## call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70be6ef",
   "metadata": {},
   "source": [
    "dit is hoe de data door de architectuur stroomt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3f2228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call(self, inputs, cross_attention_mask=None):\n",
    "    decoder_embs = inputs[0]\n",
    "    encoder_embs = inputs[1]\n",
    "\n",
    "    skip = decoder_embs\n",
    "    decoder_embs = self.masked_multi_head_attn(\n",
    "      decoder_embs, decoder_embs,  use_causal_mask=True)\n",
    "    decoder_embs = self.layer_norm_1(keras.layers.Add()([decoder_embs, skip]))\n",
    "\n",
    "    skip = decoder_embs\n",
    "    decoder_embs = self.cross_attn(\n",
    "      query=decoder_embs, value=encoder_embs, key=encoder_embs,\n",
    "      attention_mask=cross_attention_mask)\n",
    "    decoder_embs = self.layer_norm_2(keras.layers.Add()([decoder_embs, skip]))\n",
    "\n",
    "    skip = decoder_embs\n",
    "    decoder_embs = self.feed_forward(decoder_embs)\n",
    "    decoder_embs = self.layer_norm_3(keras.layers.Add()([skip, decoder_embs]))\n",
    "    return decoder_embs"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
